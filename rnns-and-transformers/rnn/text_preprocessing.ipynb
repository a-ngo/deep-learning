{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anthony/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anthony/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/anthony/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/anthony/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "Next, we need to load the data that we want to preprocess. In this example, we will use the following sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumped over the lazy dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "\n",
    "Text normalization is the process of converting text into a standard format. This involves converting all characters to lowercase and removing any punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumped over the lazy dog\n"
     ]
    }
   ],
   "source": [
    "# Convert to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "text = ''.join(c for c in text if c not in '.,;:-')\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization is the process of splitting a sentence into individual words or tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal\n",
    "\n",
    "Stopwords are common words that do not carry much meaning and can be removed from the text. We will use NLTK's list of stopwords and remove them from the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'won', 'their', 'doing', 'all', 'not', 'weren', 'each', 'ourselves', 'herself', 'only', 'but', 'she', \"shan't\", 'have', 'no', 'your', \"weren't\", 'than', \"shouldn't\", 'an', 'over', 'its', 'above', 'both', 'isn', 'he', 'a', \"doesn't\", 'you', 'of', 'him', 'here', 'm', 'did', \"it's\", \"wouldn't\", \"mightn't\", 'itself', 'more', 're', 'being', 'hers', 'now', 'again', 'before', 'ain', 'further', 'yourself', 'wasn', 'the', 'don', 'to', 'too', 'doesn', \"didn't\", 'me', \"hadn't\", 'has', 'most', 'myself', \"she's\", 'd', 'from', 'so', 'themselves', 'same', 'other', 'when', \"aren't\", 'does', 'theirs', \"haven't\", 'mightn', 'wouldn', \"mustn't\", 'why', 'do', \"that'll\", 'in', 'our', 'ours', 'her', 'any', 'be', 'on', 'these', 'and', 'my', 'some', \"you've\", \"wasn't\", 'with', 'while', 'it', 'hadn', 'once', 'how', \"won't\", 'then', 's', \"should've\", \"couldn't\", 'needn', 'is', 'o', 'shouldn', 'there', 'after', 'whom', 'because', 'out', 'until', 'was', 'this', 'or', 'that', 'having', 'those', 'just', 'as', 'very', \"hasn't\", 'will', 'for', 'i', 'couldn', 'didn', 'haven', 'mustn', 'himself', 'down', 'into', 'about', 'against', 'few', 'aren', 'had', 'at', 't', 'them', 'who', \"you'd\", 'off', 'll', \"you're\", 'can', 'they', 'yours', 'through', 'should', \"isn't\", 'am', \"don't\", 'under', 'nor', 'own', 'hasn', 'yourselves', 'shan', 've', \"needn't\", 'which', 'ma', 'below', 'where', 'y', 'are', 'we', 'his', 'what', 'between', 'during', 'such', \"you'll\", 'been', 'by', 'were', 'up', 'if'}\n",
      "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(stop_words)\n",
    "\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of reducing a word to its base or root form. We will use Porter stemmer from NLTK for stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Perform stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the process of converting a word to its base or dictionary form. We will use WordNet lemmatizer from NLTK for lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "# Perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output the results\n",
    "\n",
    "Finally, we will output the results of each step of the text preprocessing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  the quick brown fox jumped over the lazy dog\n",
      "Tokenized text:  ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
      "Filtered tokens:  ['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n",
      "Stemmed tokens:  ['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n",
      "Lemmatized tokens:  ['quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original text: \", text)\n",
    "print(\"Tokenized text: \", tokens)\n",
    "print(\"Filtered tokens: \", filtered_tokens)\n",
    "print(\"Stemmed tokens: \", stemmed_tokens)\n",
    "print(\"Lemmatized tokens: \", lemmatized_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
