{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using LSTM\n",
    "\n",
    "In this coding exercise, you will create a simple LSTM model using PyTorch to perform text classification on a dataset of short phrases. Your task is to fill in the missing parts of the code marked with `# TODO`.\n",
    "\n",
    "You need to:\n",
    "\n",
    "- Create a vocabulary to represent words as indices.\n",
    "- Tokenize, encode, and pad the phrases.\n",
    "- Convert the phrases and categories to PyTorch tensors.\n",
    "- Instantiate the LSTM model with the vocabulary size, embedding dimensions, hidden dimensions, and output dimensions.\n",
    "- Define the loss function and optimizer.\n",
    "- Train the model for a number of epochs.\n",
    "- Test the model on new phrases and print the category predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab: {'<PAD>': 0, 'great': 1, 'goal': 2, 'scored': 3, 'amazing': 4, 'touchdown': 5, 'new': 6, 'phone': 7, 'release': 8, 'latest': 9, 'laptop': 10, 'model': 11, 'tasty': 12, 'pizza': 13, 'delicious': 14, 'burger': 15}\n",
      "encoded_sentences: [[1, 2, 3], [4, 5], [6, 7, 8], [9, 10, 11], [12, 13], [14, 15]]\n",
      "max_length: 3\n",
      "padded_sentences: [[1, 2, 3], [4, 5, 0], [6, 7, 8], [9, 10, 11], [12, 13, 0], [14, 15, 0]]\n"
     ]
    }
   ],
   "source": [
    "# Phrases (textual data) and their category labels (0 for sports, 1 for technology, 2 for food)\n",
    "# Note: this data is extremely less for realistically training an LSTM model. Feel free to use\n",
    "# a relevant data source or create your own dummy data for this exercise.\n",
    "phrases = [\"great goal scored\", \"amazing touchdown\", \"new phone release\", \"latest laptop model\", \"tasty pizza\", \"delicious burger\"]\n",
    "categories = [0, 0, 1, 1, 2, 2]\n",
    "\n",
    "# TODO: Create a vocabulary to represent words as indices\n",
    "vocab = {\n",
    "    \"<PAD>\": 0,\n",
    "    \"great\": 1, \"goal\": 2, \"scored\": 3,\n",
    "    \"amazing\": 4, \"touchdown\": 5,\n",
    "    \"new\": 6, \"phone\": 7, \"release\": 8,\n",
    "    \"latest\": 9, \"laptop\": 10, \"model\": 11,\n",
    "    \"tasty\": 12, \"pizza\": 13,\n",
    "    \"delicious\": 14, \"burger\": 15\n",
    "}\n",
    "\n",
    "# TODO: Tokenize, encode, and pad phrases\n",
    "encoded_sentences = [[vocab[word] for word in phrase.split()] for phrase in phrases]\n",
    "max_length = max([len(sentence) for sentence in encoded_sentences])\n",
    "padded_sentences = [sentence + [vocab[\"<PAD>\"]] * (max_length - len(sentence)) for sentence in encoded_sentences]\n",
    "\n",
    "print(f\"vocab: {vocab}\")\n",
    "print(f\"encoded_sentences: {encoded_sentences}\")\n",
    "print(f\"max_length: {max_length}\")\n",
    "print(f\"padded_sentences: {padded_sentences}\")\n",
    "\n",
    "# TODO: Convert phrases and categories to PyTorch tensors\n",
    "inputs = torch.LongTensor(padded_sentences)\n",
    "labels = torch.LongTensor(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM model\n",
    "class PhraseClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(PhraseClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, _) = self.lstm(embedded)\n",
    "        logits = self.fc(hidden.squeeze(0))\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, Loss: 0.360628217458725\n",
      "Epoch: 200, Loss: 0.046063899993896484\n",
      "Epoch: 300, Loss: 0.016064921393990517\n",
      "Epoch: 400, Loss: 0.008466260507702827\n",
      "Epoch: 500, Loss: 0.005360834300518036\n",
      "Epoch: 600, Loss: 0.0037694808561354876\n",
      "Epoch: 700, Loss: 0.0028331864159554243\n",
      "Epoch: 800, Loss: 0.002227128716185689\n",
      "Epoch: 900, Loss: 0.0018071482190862298\n",
      "Epoch: 1000, Loss: 0.0015011801151558757\n",
      "Test predictions: tensor([[0.8854, 0.6162, 0.1435],\n",
      "        [0.1195, 0.0986, 0.9916]])\n"
     ]
    }
   ],
   "source": [
    "# TODO: Instantiate model and define loss and optimizer\n",
    "vocab_size = len(vocab)\n",
    "emedding_dim = 10\n",
    "hidden_dim = 20\n",
    "output_dim = 3 # bc of 3 categories\n",
    "\n",
    "model = PhraseClassifier(vocab_size, emedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# TODO: Train the model for a number of epochs\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(inputs.t())\n",
    "    loss = criterion(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch: {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "# TODO: Test the model on new phrases\n",
    "with torch.no_grad():\n",
    "    # test input\n",
    "    test_sentences = [\"new laptop\", \"burger\"]\n",
    "\n",
    "    # preprocessing\n",
    "    encoded_test_sentences = [[vocab[word] for word in sentence.split()] for sentence in test_sentences]\n",
    "    padded_test_sentences = [sentence + [vocab[\"<PAD>\"]] * (max_length - len(sentence)) for sentence in encoded_test_sentences]\n",
    "    test_inputs = torch.LongTensor(padded_test_sentences)\n",
    "\n",
    "    # make prediction\n",
    "    test_predictions = torch.sigmoid(model(test_inputs.t()).squeeze(1))\n",
    "    print(\"Test predictions:\", test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
