{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator and Generator implementation\n",
    "\n",
    "In this notebook, you will implement the generator and discriminator models. These models will be use in the last exercise of this lesson to train your first GAN network! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "\n",
    "The discriminator network is going to be a pretty typical linear classifier. To make this network a universal function approximator, we'll need at least one hidden layer, and these hidden layers should have one key attribute:\n",
    "> All hidden layers will have a [Leaky ReLu](https://pytorch.org/docs/stable/nn.html#torch.nn.LeakyReLU) activation function applied to their outputs.\n",
    "\n",
    "<img src='../assets/gan_network.png' width=70% />\n",
    "\n",
    "#### Leaky ReLu\n",
    "\n",
    "We should use a leaky ReLU to allow gradients to flow backwards through the layer unimpeded. A leaky ReLU is like a normal ReLU, except that there is a small non-zero output for negative input values.\n",
    "\n",
    "<img src='../assets/leaky_relu.png' width=40% />\n",
    "\n",
    "#### Output\n",
    "\n",
    "We'll also take the approach of using a more numerically stable loss function on the outputs. Recall that we want the discriminator to output a value 0-1 indicating whether an image is _real or fake_. \n",
    "> We will ultimately use [BCEWithLogitsLoss](https://pytorch.org/docs/stable/nn.html#bcewithlogitsloss), which combines a `sigmoid` activation function **and** binary cross entropy loss in one function. \n",
    "\n",
    "So, our final output layer should not have any activation function applied to it.\n",
    "\n",
    "#### Structure\n",
    "\n",
    "The discriminator takes a high dimensional input (for example, an image) and outputs a single score value. Linear layers in the discriminator should have a number of neurons such that the dimensions of their output is smaller than the dimension of their input.\n",
    "\n",
    "### First exercise\n",
    "\n",
    "Implement a discriminator network. Your network should:\n",
    "* use fully connected layer and leaky relu\n",
    "* output a single logit\n",
    "* take a image as input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator model:\n",
    "    args: \n",
    "    - input_dim: dimension of the input data. For example, for a 28 by 28 grayscale image, the input size is 784\n",
    "    - hidden_dim: a parameter that controls the dimensions of the hidden layers. \n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #### \n",
    "        # IMPLEMENT HERE\n",
    "        ####\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim // 2)\n",
    "        self.fc2 = nn.Linear(input_dim // 2, hidden_dim // 4)\n",
    "        self.fc3 = nn.Linear(input_dim // 4, 1)\n",
    "\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #### \n",
    "        # IMPLEMENT HERE\n",
    "        ####\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "# m = nn.Softmax(dim=1)\n",
    "# input = torch.randn(2, 3)\n",
    "# output = m(input)\n",
    "\n",
    "# print(input)\n",
    "# print(output)\n",
    "# print(8 // 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x128 and 392x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/anthony/workspace/deep-learning/generative-adversarial-networks/basics/MNIST_gan_generator_discriminator/MNIST_GAN_generator_discriminator_Starter.py.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anthony/workspace/deep-learning/generative-adversarial-networks/basics/MNIST_gan_generator_discriminator/MNIST_GAN_generator_discriminator_Starter.py.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m hidden_dim \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/anthony/workspace/deep-learning/generative-adversarial-networks/basics/MNIST_gan_generator_discriminator/MNIST_GAN_generator_discriminator_Starter.py.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m discriminator \u001b[39m=\u001b[39m Discriminator(input_dim, hidden_dim)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anthony/workspace/deep-learning/generative-adversarial-networks/basics/MNIST_gan_generator_discriminator/MNIST_GAN_generator_discriminator_Starter.py.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tests\u001b[39m.\u001b[39;49mcheck_discriminator(discriminator, input_dim)\n",
      "File \u001b[0;32m~/workspace/deep-learning/generative-adversarial-networks/basics/MNIST_gan_generator_discriminator/tests.py:8\u001b[0m, in \u001b[0;36mcheck_discriminator\u001b[0;34m(model, input_dim)\u001b[0m\n\u001b[1;32m      6\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m16\u001b[39m\n\u001b[1;32m      7\u001b[0m model_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(batch_size, input_dim)\n\u001b[0;32m----> 8\u001b[0m \u001b[39massert\u001b[39;00m model(model_input)\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mSize([batch_size, \u001b[39m1\u001b[39m]), \\\n\u001b[1;32m      9\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mYour model should output a single score for each element in the batch\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCongrats, you successfully implemented your discriminator\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/anthony/workspace/deep-learning/generative-adversarial-networks/basics/MNIST_gan_generator_discriminator/MNIST_GAN_generator_discriminator_Starter.py.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anthony/workspace/deep-learning/generative-adversarial-networks/basics/MNIST_gan_generator_discriminator/MNIST_GAN_generator_discriminator_Starter.py.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anthony/workspace/deep-learning/generative-adversarial-networks/basics/MNIST_gan_generator_discriminator/MNIST_GAN_generator_discriminator_Starter.py.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/anthony/workspace/deep-learning/generative-adversarial-networks/basics/MNIST_gan_generator_discriminator/MNIST_GAN_generator_discriminator_Starter.py.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anthony/workspace/deep-learning/generative-adversarial-networks/basics/MNIST_gan_generator_discriminator/MNIST_GAN_generator_discriminator_Starter.py.ipynb#W4sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/anthony/workspace/deep-learning/generative-adversarial-networks/basics/MNIST_gan_generator_discriminator/MNIST_GAN_generator_discriminator_Starter.py.ipynb#W4sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x128 and 392x64)"
     ]
    }
   ],
   "source": [
    "# for a 28x28 grayscale image flattened, the input dim is 784\n",
    "input_dim = 784\n",
    "hidden_dim = 256\n",
    "\n",
    "discriminator = Discriminator(input_dim, hidden_dim)\n",
    "tests.check_discriminator(discriminator, input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "The generator network will be almost exactly the same as the discriminator network, except that we're applying a [tanh activation function](https://pytorch.org/docs/stable/nn.html#tanh) to our output layer.\n",
    "\n",
    "#### tanh Output\n",
    "The generator has been found to perform the best with $tanh$ for the generator output, which scales the output to be between -1 and 1, instead of 0 and 1. \n",
    "\n",
    "<img src='../assets/tanh_fn.png' width=40% />\n",
    "\n",
    "Recall that we also want these outputs to be comparable to the *real* input pixel values, which are read in as normalized values between 0 and 1. \n",
    "> So, we'll also have to **scale our real input images to have pixel values between -1 and 1** when we train the discriminator. \n",
    "\n",
    "## Second Exercise\n",
    "Implement a generator network. Your network should:\n",
    "* use fully connected, leaky relu and tanh layers\n",
    "* take a latent as input\n",
    "* output a vector (we will later reshape it as an image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim: int, hidden_dim: int, output_size: int):\n",
    "        super(Generator, self).__init__()\n",
    "        #### \n",
    "        # IMPLEMENT HERE\n",
    "        ####\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #### \n",
    "        # IMPLEMENT HERE\n",
    "        ####\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "hidden_dim = 256\n",
    "output_dim = 784\n",
    "\n",
    "generator = Generator(latent_dim, hidden_dim, output_dim)\n",
    "tests.check_generator(generator, latent_dim, output_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
